<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Homework 1: Rasterizer</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        img {
            max-width: 100%;
            height: auto;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
            vertical-align: middle;
        }
        th {
            background-color: #f2f2f2;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
        }
        .table-3-col td img {
            width: calc(100% - 10px);
            height: auto;
            display: block;
            margin: 0 auto;
        }
        .table-4-col td img {
            width: calc(100% - 10px);
            height: auto;
            display: block;
            margin: 0 auto;
        }
        .single-image {
            max-width: 800px;
            margin: 20px auto;
            display: block;
        }
        .math-equation {
            overflow-x: auto;
            padding: 10px 0;
            text-align: center;
        }
    </style>
</head>
<body>
    <h1>Rasterizer Implementation, Supersampling and Texture Mapping</h1>
    <blockquote>
        <p>Brayton Lordianto and Max Wilcoxson (Team Vis10n Pro)</p>
    </blockquote>
    
    <p><a href="https://cs184.eecs.berkeley.edu/sp25/hw/hw1/">Project Specs</a></p>
        
    <h1>Report</h1>
    
    <h2>Overview</h2>
    
    <p>We implemented a basic rasterizer capable of rendering triangles, applying supersampling for anti-aliasing, performing affine transformations, interpolating colors using barycentric coordinates, and mapping textures with different sampling methods. We explored various techniques to improve the visual quality of rendered images, including supersampling, bilinear interpolation, and mipmapping. The tasks spanned across different levels of the rasterization pipeline, and allowed us to gain understanding of the rasterization process. The most interesting takeaway was directly seeing how intelligently sampling textures at the appropriate frequency via level sampling from the mipmap led to significantly reduced aliasing with much smaller additional computational overhead compared to supersampling.
    
    <h2>Task 1: Rasterizing Triangles</h2>
    
    <p>The rasterizer is a simple algorithm that takes in a triangle and renders it to a framebuffer. The main pain points of the task is how to handle the rasterization of a single triangle given the coordinates of the vertices irrespective of the winding order. To do this, we first calculate the bounding box of the triangle. The bounding box of the triangle is defined as the smallest rectangle that can be drawn whilst ensuring that the entire triangle is within it.</p>
    
    <p>Then, for consistency, we converted the order of the vertices to be counter-clockwise. This is done by checking the cross product of the vectors formed by vertices. While there are several equivalent formulations, we choose to take the cross product <code>p1 - p0</code> and <code> p2 - p0</code>, and swap order if this cross product is negative. Let's try to understand why this is the case. Notice that the basis vectors are ordered in the x-axis then the y-axis, that is <code>(1, 0)</code> and <code>(0, 1)</code>, and the area is positive here. Since the said basis vectors are ordered in the counter-clockwise direction, it means that the area is positive if the vectors are ordered in the counter-clockwise direction. The cross product in 2D is the determinant of the matrix formed by the vectors, or specifically the signed area of the parallelogram formed by the two basis vectors in 2D are transformed to the first and second vectors. So if the cross product is negative, it means that the two vectors are ordered in the clockwise direction, and we need to swap them to make it counter-clockwise.</p>
    
    <p>Now that we have points in counter-clockwise order, we can be sure that a point <code>p</code> is in the triangle if and only if the edge equation <code>L</code> is positive for all edges. The best way to interpret this is that the normal for any 2 points in counter-clockwise order is pointing inwards to the third point. So if the point is inside the triangle, the dot product of the normal and the vector from the point to the edge should be positive (this is how we calculate <code>L</code>). This is why we can use the edge equations to determine if a point is inside the triangle.</p>
    
    <p>So we iterate over all the pixels in the bounding box of the triangle, and for each pixel, we check if the center of the pixel is inside the triangle. If it is, then we set the pixel to the color of the triangle by storing it in the framebuffer. NOTE: this algorithm is no worse than one that checks each sample within the bounding box of the triangle, since we are only checking the center of each pixel in the bounding box. We are not checking any pixels outside the bounding box.</p>
    
    <h3>Results</h3>
    <p>This is an example of some triangles rendered with the rasterizer, and a closeup zoom on an interesting part of the image, which is the tip of the thin purple triangle. Since we have not enabled more complicated sampling techniques, we see some aliasing, where the continuous triangle does not look continuous. </p>
    
    <img src="images/samplerate1.png" alt="Sample rate 1" class="single-image">
    
    <h2>Task 2: Supersampling</h2>
    
    <p>When rendering triangles, aliasing creates "jaggies" (look at the above image to see them on the thin triangle). This is because the rasterizer only samples the color at the center of each pixel even though the triangle could cover the a significant portion of the pixel not covered by the center. With supersampling, we can sample the color at multiple points within the pixel and average the results to get a better approximation of the color. This reduces the jaggies and makes the image look smoother. The idea is illustrated in the following image, where we sample at 4 points within the pixel and average the results.</p>
    
    <img src="images/image-4.png" alt="Supersampling illustration" class="single-image">
    
    <p>Our data structure changes to simple rasterization without supersampling include modifying the sample buffer to allow for multiple samples per pixel. We did this by resizing the sample buffer to be the size of the image multiplied by the number of samples per pixel.</p>
    
    <pre><code>this->sample_buffer.resize(width * height * rate, Color::White);</code></pre>
    
    <p>We then kept the samples as a contiguous array. We then keep a <em>sampling factor</em> <code>s_factor</code> to represent the subsamples we take in each dimension.</p>
    
    <p>We added 2 additional functions, one to store the sample and one to get the sample. These functions handle the mapping between the 2D pixel coordinates and the 1D sample buffer. This is useful to avoid bugs with different methods accidentally indexing into the buffer in different ways. </p>
    
    <p>Additionally, we had to scale all my triangle coordinates by the sampling factor to account for the increased resolution. This was done in the <code>rasterize_triangle</code> function. This includes the bounding box of the triangle. But for the edge test, we had to calculate <code>l</code> in screen space, so we had to divide by the sampling factor. That is why we had to do <code>(i + 0.5) / s_factor</code> instead of <code>(i + 0.5)</code>.</p>
    
    <p>Finally, when we actually rasterize the sample buffer to the framebuffer, we need to average the subsampled pixels. To accomplish this, whenever a single point is resolved to the framebuffer in <code>resolve_to_framebuffer</code>, we look at the surrounding subsamples and average the colors. </p>

    <p> Supersampling is useful because it allows us to more precisely estimate when triangles start and end. It accomplishes this by effectively rendering the triangle at a higher resolution, and then downsampling to the framebuffer size through averaging. </p>
    
    <h3>Results and Discussion</h3>
    
    <p>Here is the image zoomed in on the tip of the triangle.</p>
    <table class="table-3-col">
        <tr>
            <th>Sample Rate 1</th>
            <th>Sample Rate 4</th>
            <th>Sample Rate 16</th>
        </tr>
        <tr>
            <td><img src="images/srate1.png" alt="Sample rate 1" style="height: 500px !important;"></td>
            <td><img src="images/srate2.png" alt="Sample rate 4" style="height: 500px !important;"></td>
            <td><img src="images/srate3.png" alt="Sample rate 16" style="height: 500px !important;"></td>
        </tr>
    </table>

</body>
</html>    
    <p>Here is the image zoomed in and captured using the GUI, with the pixel inspector zoomed in on the tip of the thin triangle.</p>
    
    <table class="table-3-col">
        <tr>
            <th>Sample Rate 1</th>
            <th>Sample Rate 4</th>
            <th>Sample Rate 16</th>
        </tr>
        <tr>
            <td><img src="images/samplerate1.png" alt="Sample rate 1 zoomed" style="width: 50%; height: 10%;"></td>
            <td><img src="images/samplerate4.png" alt="Sample rate 4 zoomed" style="width: 50%; height: 10%;"></td>
            <td><img src="images/samplerate16.png" alt="Sample rate 16 zoomed" style="width: 50%; height: 10%;"></td>
        </tr>
    </table>
    
    <p>We can see that the image gets smoother as the sample rate increases and there are less jaggies. This is because we are sampling the color at multiple points within the pixel and averaging the results, which reduces aliasing by simply sampling at a higher frequency to increase the range of frequencies that we can capture.</p>
    
    <h2>Task 3: Transforms</h2>
    
    <p>We create <strong>Affine Transformations</strong> to transform the vertices of the triangles. First, the main way this is done is, given a point <code>(x,y)</code>, we can transform it to a new point <code>(x',y')</code> by converting <code>(x,y)</code> to homogeneous coordinates <code>(x,y,1)</code>, then multiplying it by a 3x3 matrix <code>M</code> to get <code>(x',y',w')</code>, and then converting back to cartesian coordinates <code>(x'/w',y'/w')</code>. This is done implicitly in the overloaded <code>*</code> operator when you multiply a 2d vector by a 3x3 matrix. Then what is left is to define the 3x3 matrix <code>M</code> for each transformation.</p>
    
    <p>We played around with some transforms of static objects. Check it out!</p>
    
    <table class="table-2-col">
        <tr>
            <th>Simple Transforms</th>
            <th>Custom Transforms and Color Scheme</th>
        </tr>
        <tr>
            <td><img src="images/robot.png" alt="Simple robot transforms"></td>
            <td><img src="images/myrobot.png" alt="Custom robot transforms"></td>
        </tr>
    </table>
    
    <p>In the second image, we were trying to make the cube man look like it is thinking... but abstract art contains multitudes of interpretations!</p>
    
    <h2>Task 4: Barycentric coordinates</h2>
    
    <p>So far, the pixels of triangles are colored uniformly. Practically, we want to allow the inside of the triangles to be colored differently while being a smooth change. In graphics, we do this by attaching color to the vertices (this would be passed in as a parameter attached to every vertex and accessible in the shader) and then interpolating the color of the pixel based on the barycentric coordinates of the pixel.</p>
    
    <p>Barycentric coordinates are a way to represent a point in a triangle as a <strong>linear combination</strong> of the vertices of the triangle. This is done by finding the weights <code>u</code>, <code>v</code>, and <code>w</code> such that <code>u + v + w = 1</code> and <code>u*v1 + v*v2 + w*v3 = p</code>, where <code>v1</code>, <code>v2</code>, and <code>v3</code> are the vertices of the triangle and <code>p</code> is the point. This allows us to compute the color of each pixel as dot product of its barycentric coordinates and the color corresponding to each vertex. Below is an image showing how the barycentric coordinates define the color of the triangle. Notice how the middle of the triangle is the combination of the 3 colors. The second image on the right is a color wheel using barycentric coordinates <strong> rendered with our rasterizer</strong>.</p>
    
    <table class="table-2-col">
        <tr>
            <td><img src="images/image-8.png" alt="Barycentric coordinates example"></td>
            <td><img src="images/image-9.png" alt="Color wheel using barycentric coordinates"></td>
        </tr>
    </table>
    
    <p>To calculate the barycentric coordinates relative to vertex <code>0</code> of the triangle, we take the area of the triangle formed by the point<code>P</code> with vertices <code>1</code> and <code>2</code>, and divide it by the area of the triangle. This intuitively makes sense, since the area of each subtriangle will add up to the area of the whole triangle, so these coordinates sum to 1. The value of a particularly vertex's barycentric coordinate also increases linearly with the area of the corresponding subtriangle. A visualization of this is shown below.</p>
    
    <img src="images/image-10.png" alt="Barycentric coordinates calculation" class="single-image">
    
    <p>Similar to computing the area of the triangle formed by two vectors using cross products from Task 1, we can also compute these areas with cross products of the two vectors forming each triangle, given in counterclockwise order so the cross product is positive. </p>
    
    <h2>Task 5: "Pixel sampling" for texture mapping</h2>
    
    <p>The goal here is to perform texture mapping. To do so, we are still given coordinates of a triangle, and then we are given the corresponding texture coordinates of the triangle. These texture coordinates are coordinates in the texture image space while the triangle coordinates are in screen space.</p>
    
    <p>The first step is to convert our coordinates from screen space to texture space. The insight here is that both coordiantes are the same in barycentric coordinates. So we can get the barycentric weights of the pixel in screen space, and then use those weights to interpolate the texture coordinates to get the texture coordinates of the pixel. This is done by doing a linear combination of the texture coordinates and the barycentric weights.</p>
    
    <h3>Method 1: Nearest Neighbor Sampling</h3>
    <p>However, in the texture space, it is more likely that the coordinate we get is not at the center of a pixel. One way to handle this is to sample the nearest pixel. This is called <strong>nearest neighbor sampling</strong>. To do this, we simply access the color of the nearest pixel at the right level of the mipmap (we will discuss mipmaps later).</p>

    <p>To do this, we first have the coordiantes in <code>uv</code> space for the texture. Then knowing the <code>XY</code> width and height of the texture, we can convert this to the <code>XY</code> coordinates of the texture. The equation however is not straightforward. At first, we might think to multiply <code>u, v</code> by <code>x - 1, y - 1</code>, subtract 0.5, then round it. However, there is an edge case if we want to access <code>u = 0</code> or <code> v = 0</code>. This would give <code> round(-0.5) = -1 </code> due to rounding behavior of negative numbers in Python. This results in first having to add <code>1</code> before rounding, then subtracting <code>1</code> after rounding. So we get the following in <code>XY</code> space of the texture:</p>

    <pre><code>round(1 + uv.x*(width - 1) - 0.5) - 1; 
round(1 + uv.y*(height - 1) - 0.5) - 1;</code></pre>

    <p>We then access the texture at the right level of the mipmap.</p>

    <h3>Method 2: Bilinear Interpolation</h3>
    <p>However, if we only sample without any other considerations, we will get aliasing of jaggies. One solution that helps reduce jaggies is <strong>bilinear interpolation</strong>. The below image shows the difference between nearest neighbor sampling and bilinear interpolation with clear jaggies for nearest neighbor, while the second image illustrates how bilinear interpolation works.</p>

    <table class="table-2-col">
        <tr>
            <td><img src="images/image-12.png" alt="Sampling comparison"></td>
            <td><img src="images/image-13.png" alt="Bilinear interpolation illustration"></td>
        </tr>
    </table>

    <p>As shown in the second image, this is done by sampling the 4 nearest pixels and then interpolating the color based on the distance to the 4 pixels. Then we linearly interpolate the color based on the distance to the 4 pixels if they exist.</p>

    <h3>Results</h3>
    <p>The difference between nearest neighbor sampling and bilinear interpolation can be seen in the following images. The first image is nearest neighbor sampling and the second image is bilinear interpolation. We can see the white grid lines clearly in the second image, whereas it looks cut off from the rest of the grid lines in the first image. This is because the second image is interpolating the color.</p>

    <p><strong>sample rate 1</strong></p>
    <table class="table-2-col">
        <tr>
            <th>Nearest Neighbor Sampling</th>
            <th>Bilinear Interpolation</th>
        </tr>
        <tr>
            <td><img src="images/image-14.png" alt="Nearest neighbor at sample rate 1"></td>
            <td><img src="images/image-15.png" alt="Bilinear at sample rate 1"></td>
        </tr>
    </table>

    <p>These are both in sample rate 1. However, note that supersampling can also help to reduce the same aliasing effect that bilinear interpolation does. Below is an example of both methods at sample rate 16. The difference is not as visible in the higher sampling rate regime, but definitely still present. Bilinear interpolation is still better as it is more visually consistent and smooth showing the white grid.</p>

    <p><strong>sample rate 16</strong></p>
    <table class="table-2-col">
        <tr>
            <th>Nearest Neighbor Sampling</th>
            <th>Bilinear Interpolation</th>
        </tr>
        <tr>
            <td><img src="images/image-16.png" alt="Nearest neighbor at sample rate 16"></td>
            <td><img src="images/image-17.png" alt="Bilinear at sample rate 16"></td>
        </tr>
    </table>

    <p>There will be a large difference between the two methods in general when we are magnifying pixels as bilinear interpolation will smooth it out, and when there are obvious jaggies, since bilinear interpolation will smooth it out. There will also be a large difference on high contrast boundaries, as nearest neighbor sampling will show the boundary clearly, while bilinear interpolation will create a gradient of intermediate values, reducing the contrast but making it look smoother. The core difference is that nearest picks a single texel value, while bilinear averages four neighboring texels. This averaging creates smoother transitions but can blur fine details.</p>

    <h2>Task 6: "Level sampling" with mipmaps for texture mapping</h2>

    <p>Point sampling unfortunately still causes aliasing such as Moire effects for far away textures. This is because the texture is being sampled at a single point at a single resolution, and the texture is being stretched out. The amount of stretching of the texture (and underlying resolution of the texture) is likely to not be consistent throughout the entire image, making it difficult to have one sampling rate that works for the entire image. Here is an example (from class lecture slides) that shows the aliasing effect when simply doing point sampling:</p>

    <img src="images/image-11.png" alt="Moire effects example" class="single-image">

    <p>The solution to this is to use <strong>Mipmaps</strong> and sample at different levels of the mipmap. Mipmaps are a series of images that are each half the resolution of the previous image. This is done by repeatedly downsampling the image by a factor of 2. For areas in texture space that span very small areas in screen space, we should sample at a level of mipmap that is more zoomed in. For areas in texture space that span very large areas in screen space, we should sample at a level of mipmap that is more zoomed out.</p>

    <img src="images/image-19.png" alt="Mipmap levels" class="single-image">

    <h3>Understanding MIPMAP level calculation</h3>
    <p>Essentially, every single pixel in screen space not only maps to a pixel in texture space, but it also maps to a level of the mipmap. We can calculate this mipmap level with a formula. But to understand the formula, it is important to intuitively recognize the following. Let us define higher level mipmaps as more coarse and lower level mipmaps as more fine (or zoomed in). Then, if we consider a single pixel <code>(px, py)</code> and <code>(px+1, py)</code> and <code>(px, py+1)</code> in screen space then calculate their corresponding texture coordinates, we have a measure of how much the texture is being stretched out at that point. In other words, we have the "footprint" of the pixel in the <code>x</code> and <code>y</code> direction in texture space. The larger the footprint, the more the texture is being stretched out, and the more coarse the mipmap level should be.</p>

    <p>The math behind it is described as follows:</p>
    <img src="images/image-18.png" alt="Mipmap level calculation" class="single-image">

    <p>The differential norms measure the footprint of the pixel in the <code>x</code> and <code>y</code> direction in texture space. The <code>max</code> function is used to take the largest footprint to account for the worst case (most stretching) direction. The <code>log2</code> function is used to convert the footprint to a level of the mipmap. Specifically, level 0 when footprint is <code><=1</code>, level 1 when footprint is <code><=2</code>, level 2 when footprint is <code><=4</code>, and so on. We implement this as a function <code>get_level</code> as part of the <code>Texture</code> class.</p>

    <h3>Methodology</h3>
    <p>Therefore, we first calculate the texture uv coordinates of the pixel in screen space, and of the pixel to the right and below it. Then we calculate the footprint of the pixel in the <code>x</code> and <code>y</code> direction in texture space. We then have to scale the footprint by the width and height of the texture to get the footprint in texture xy space.</p>

    <p>Then we calculate the mipmap level using the formula above. Then we sample the texture at the calculated mipmap level according to either nearest neighbor sampling or bilinear interpolation.</p>

    <h3>Sampling Methods and Tradeoffs</h3>
    <p>We also implemented three different Level Sampling methods. The first is <em>zero</em>, which means we sample from the zero level mipmap, not taking into account the footprint. The second is <em>nearest</em>, which means we sample from the nearest mipmap level, since the level we calculate is not necessarily an integer. The third is <em>linear</em>, which means we sample from the two nearest mipmap levels and linearly interpolate between them.</p>

    <p>There are tradeoffs between the three techniques of level sampling in terms of speed, memory usage, and antialiasing power. Specifically, <em>zero</em> is the fastest and uses the least memory, but it does not perform antialiasing. <em>linear</em> is the slowest and uses the most memory, but it most accurately picks the appropriate mipmap level, leading to stronger antialiasing performance. <em>nearest</em> is in between the two in terms of speed and memory usage, but it antialiases more effectively than <em>zero</em> but less effectively than <em>linear</em>.</p>

    <p> In terms of texture sampling methods, we found <code>bilinear</code> performs better at antialiasing by interpolating adjacent pixels, but requires more computational power and runtime than the simpler <code>nearest</code> approach to texture sampling by just picking the nearest neighbor.</p>

    <p> Finally, in terms of level of supersampling, higher rates of supersampling improve antialiasing performance, but require linearly increasing performance costs with the number of subsamples per pixel. </p>

    <h3>Results and Discussion</h3>

    <p>In the following images, we show combinations of different level sampling methods (zero and linear) and different pixel sampling methods (nearest neighbor and bilinear). We see that Moire aliasing is a problem for all rendered versions of this image, however the aliasing is most effectively reduced with <code>P_LINEAR</code> texture sampling and <code>L_LINEAR</code> level interpolation. All images are shown with no supersampling (ie. samples per pixel is 1).</p>

    <p><em>Key: <code>L</code> is level sampling method, <code>P</code> is pixel sampling method. <code>L_ZERO</code> is zero, <code>L_NEAREST</code> is nearest, <code>L_LINEAR</code> is linear. <code>P_NEAREST</code> is nearest neighbor, <code>P_LINEAR</code> is bilinear interpolation.</em></p>

    <table class="table-4-col">
        <tr>
            <th>L_ZERO and P_NEAREST</th>
            <th>L_ZERO and P_LINEAR</th>
            <th>L_NEAREST and P_NEAREST</th>
            <th>L_NEAREST and P_LINEAR</th>
        </tr>
        <tr>
            <td><img src="images/l_zero_p_nearest.png" alt="L_ZERO and P_NEAREST"></td>
            <td><img src="images/l_zero_p_linear.png" alt="L_ZERO and P_LINEAR"></td>
            <td><img src="images/l_nearest_p_nearest.png" alt="L_NEAREST and P_NEAREST"></td>
            <td><img src="images/l_nearest_p_linear.png" alt="L_NEAREST and P_LINEAR"></td>
        </tr>
    </table>