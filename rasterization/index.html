<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Homework 1: Rasterizer</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        img {
            max-width: 100%;
            height: auto;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
            vertical-align: middle;
        }
        th {
            background-color: #f2f2f2;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
        }
        .table-3-col td img {
            width: calc(100% - 10px);
            height: auto;
            display: block;
            margin: 0 auto;
        }
        .table-4-col td img {
            width: calc(100% - 10px);
            height: auto;
            display: block;
            margin: 0 auto;
        }
        .single-image {
            max-width: 800px;
            margin: 20px auto;
            display: block;
        }
        .math-equation {
            overflow-x: auto;
            padding: 10px 0;
            text-align: center;
        }
    </style>
</head>
<body>
    <h1>Rasterizer Implementation, Supersampling and Texture Mapping</h1>
    <blockquote>
        <p>Brayton Lordianto and Max Wilcoxson (Team Vis10n Pro)</p>
    </blockquote>
    
    <p><a href="https://cs184.eecs.berkeley.edu/sp25/hw/hw1/">Project Specs</a></p>
        
    <h1>Report</h1>
    
    <h2>Overview</h2>
    
    <p>We implemented a basic rasterizer capable of rendering triangles, applying supersampling for anti-aliasing, performing affine transformations, interpolating colors using barycentric coordinates, and mapping textures with different sampling methods. We explored various techniques to improve the visual quality of rendered images, including supersampling, bilinear interpolation, and mipmapping. The tasks spanned across different levels of the rasterization pipeline, providing a comprehensive understanding of the fundamental concepts in computer graphics.</p>
    
    <h2>Task 1: Rasterizing Triangles</h2>
    
    <p>The rasterizer is a simple algorithm that takes in a triangle and renders it to a framebuffer. The main pain points of the task is how to handle the rasterization of a single triangle given the coordinates of the vertices irrespective of the winding order. To do this, we first calculate the bounding box of the triangle. The bounding box of the triangle is defined as the smallest rectangle that can be drawn whilst ensuring that the entire triangle is within it.</p>
    
    <p>Then, for consistency, we converted the order of the vertices to be counter-clockwise. This is done by checking the cross product of the vectors formed by the vertices, specifically the first and the second. If the cross product is negative, we swap the vertices. Let's try to understand why this is the case. Notice that the basis vectors are ordered in the x-axis then the y-axis, that is <code>(1, 0)</code> and <code>(0, 1)</code>, and the area is positive here. Since the said basis vectors are ordered in the counter-clockwise direction, it means that the area is positive if the vectors are ordered in the counter-clockwise direction. The cross product in 2D is the determinant of the matrix formed by the vectors, or specifically the signed area of the parallelogram formed by the two basis vectors in 2D are transformed to the first and second vectors. So if the cross product is negative, it means that the two vectors are ordered in the clockwise direction, and we need to swap them to make it counter-clockwise.</p>
    
    <p>Now that we have points in counter-clockwise order, we can be sure that a point <code>p</code> is in the triangle if and only if the edge equation <code>L</code> is positive for all edges. The best way to interpret this is that the normal for any 2 points in counter-clockwise order is pointing inwards to the third point. So if the point is inside the triangle, the dot product of the normal and the vector from the point to the edge should be positive (this is how we calculate <code>L</code>). This is why we can use the edge equations to determine if a point is inside the triangle.</p>
    
    <p>So we iterate over all the pixels in the bounding box of the triangle, and for each pixel, we check if the the pixel is inside the triangle. If it is, then we set the pixel to the color of the triangle by storing it in the framebuffer. NOTE: this algorithm is no worse than one that checks each sample within the bounding box of the triangle, since we are only checking the center of each pixel in the bounding box. We are not checking any pixels outside the bounding box.</p>
    
    <h3>Results</h3>
    <p>This is an example of some triangles rendered with the rasterizer, and a closeup zoom on an interesting part of the image, which is the tip of the thin blue triangle.</p>
    
    <img src="images/samplerate1.png" alt="Sample rate 1" class="single-image">
    
    <h2>Task 2: Supersampling</h2>
    
    <p>When rendering triangles, aliasing creates "jaggies" (look at the above image to see them on the thin triangle). This is because the rasterizer only samples the color at the center of each pixel even though the triangle could cover the a significant portion of the pixel not covered by the center. With supersampling, we can sample the color at multiple points within the pixel and average the results to get a better approximation of the color. This reduces the jaggies and makes the image look smoother. The idea is illustrated in the following image, where we sample at 4 points within the pixel and average the results.</p>
    
    <img src="images/image-4.png" alt="Supersampling illustration" class="single-image">
    
    <p>Our data structure changes to simple rasterization without supersampling include modifying the sample buffer to allow for multiple samples per pixel. We did this by resizing the sample buffer to be the size of the image multiplied by the number of samples per pixel.</p>
    
    <pre><code>this->sample_buffer.resize(width * height * rate, Color::White);</code></pre>
    
    <p>We then kept the samples as a contiguous array. We then keep a <em>sampling factor</em> <code>s_factor</code> to represent the subsamples we take in each dimension.</p>
    
    <p>We added 2 additional functions, one to store the sample and one to get the sample. These functions handle the mappig between the 2D pixel coordinates and the 1D sample buffer.</p>
    
    <p>Finally, we had to average the samples. So whenever you rasterize a single point, we look at the surrounding subsamples instead and average the colors. This is done in the <code>resolve_to_framebuffer</code> function.</p>
    
    <h3>Challenges</h3>
    <p>The trickiest part was getting the indexing right in the sample buffer and ensuring the edge equations properly handled the supersampled coordinates. This implementation gives us better antialiasing by essentially rendering at a higher resolution and then downsampling with averaging.</p>
    
    <h3>Changes to the rasterizer</h3>
    <p>Additionally, We had to scale all my triangle coordinates by the sampling factor to account for the increased resolution. This was done in the <code>rasterize_triangle</code> function. This includes the bounding box of the triangle. But for the edge test, we had to calculate <code>l</code> in screen space, so we had to divide by the sampling factor. That is why we had to do <code>(i + 0.5) / s_factor</code> instead of <code>(i + 0.5)</code>.</p>
    
    <h3>Results and Discussion</h3>
    
    <p>Here is the image zoomed in on the tip of the triangle.</p>
    <table class="table-3-col">
        <tr>
            <th>Sample Rate 1</th>
            <th>Sample Rate 4</th>
            <th>Sample Rate 16</th>
        </tr>
        <tr>
            <td><img src="images/srate1.png" alt="Sample rate 1" style="height: 500px !important;"></td>
            <td><img src="images/srate2.png" alt="Sample rate 4" style="height: 500px !important;"></td>
            <td><img src="images/srate3.png" alt="Sample rate 16" style="height: 500px !important;"></td>
        </tr>
    </table>

</body>
</html>    
    <p>Here is the image zoomed in and captured using the GUI, with the pixel inspector zoomed in on the tip of the thin triangle.</p>
    
    <table class="table-3-col">
        <tr>
            <th>Sample Rate 1</th>
            <th>Sample Rate 4</th>
            <th>Sample Rate 16</th>
        </tr>
        <tr>
            <td><img src="images/samplerate1.png" alt="Sample rate 1 zoomed" style="width: 50%; height: 10%;"></td>
            <td><img src="images/samplerate4.png" alt="Sample rate 4 zoomed" style="width: 50%; height: 10%;"></td>
            <td><img src="images/samplerate16.png" alt="Sample rate 16 zoomed" style="width: 50%; height: 10%;"></td>
        </tr>
    </table>
    
    <p>We can see that the image gets smoother as the sample rate increases and there are less jaggies. This is because we are sampling the color at multiple points within the pixel and averaging the results, which by definition reduces aliasing.</p>
    
    <h2>Task 3: Transforms</h2>
    
    <p>We create <strong>Affine Transformations</strong> to transform the vertices of the triangles. First, the main way this is done is, given a point <code>(x,y)</code>, we can transform it to a new point <code>(x',y')</code> by converting <code>(x,y)</code> to homogeneous coordinates <code>(x,y,1)</code>, then multiplying it by a 3x3 matrix <code>M</code> to get <code>(x',y',w')</code>, and then converting back to cartesian coordinates <code>(x'/w',y'/w')</code>. This is done implicitly in the overloaded <code>*</code> operator when you multiply a 2d vector by a 3x3 matrix. Then what is left is to define the 3x3 matrix <code>M</code> for each transformation.</p>
    
    <p>I played around with some transforms of static objects. Check it out!</p>
    
    <table class="table-2-col">
        <tr>
            <th>Simple Transforms</th>
            <th>A little more custom transforms and different color scheme</th>
        </tr>
        <tr>
            <td><img src="images/robot.png" alt="Simple robot transforms"></td>
            <td><img src="images/myrobot.png" alt="Custom robot transforms"></td>
        </tr>
    </table>
    
    <p>In the second image, I was trying to make the cube man look like it is thinking... But can be interpreted in other ways!</p>
    
    <h2>Task 4: Barycentric coordinates</h2>
    
    <p>So far, the pixels of triangles are colored uniformly. Practically, we want to allow the inside of the triangles to be colored differently while being a smooth change. In graphics, we do this by attaching color to the vertices (this would be passed in as a parameter attached to every vertex and accessible in the shader) and then interpolating the color of the pixel based on the barycentric coordinates of the pixel.</p>
    
    <p>Barycentric coordinates are a way to represent a point in a triangle as a <strong>linear combination</strong> of the vertices of the triangle. This is done by finding the weights <code>u</code>, <code>v</code>, and <code>w</code> such that <code>u + v + w = 1</code> and <code>u*v1 + v*v2 + w*v3 = p</code>, where <code>v1</code>, <code>v2</code>, and <code>v3</code> are the vertices of the triangle and <code>p</code> is the point. This allows us to interpolate the color of the pixel based on the barycentric coordinates of the pixel. Below is an image showing how the barycentric coordinates define the color of the triangle. Notice how the middle of the triangle is the combination of the 3 colors. The second image on the right is a color wheel using barycentric coordinates.</p>
    
    <table class="table-2-col">
        <tr>
            <td><img src="images/image-8.png" alt="Barycentric coordinates example"></td>
            <td><img src="images/image-9.png" alt="Color wheel using barycentric coordinates"></td>
        </tr>
    </table>
    
    <p>To calculate the barycentric coordinates relative to point <code>0</code> of the triangle, we basically take the area of the triangle formed by the point with point <code>0</code> and <code>1</code>, and divide it by the area of the triangle formed by the point with point <code>1</code> and <code>2</code>. A visualization of this is shown below.</p>
    
    <img src="images/image-10.png" alt="Barycentric coordinates calculation" class="single-image">
    
    <p>We are kind of taking the shaded area as a ratio of the total area of the triangle. This is done using cross products with the point of the pixel over the cross product of the vertices of the triangle.</p>
    
    <h2>Task 5: "Pixel sampling" for texture mapping</h2>
    
    <p>The goal here is to perform texture mapping. To do so, we are still given coordinates of a triangle, and then we are given the corresponding texture coordinates of the triangle. These texture coordinates are coordinates in the texture image space while the triangle coordinates are in screen space.</p>
    
    <p>The first step is to convert our coordinates from screen space to texture space. The insight here is that both coordiantes are the same in barycentric coordinates. So we can get the barycentric weights of the pixel in screen space, and then use those weights to interpolate the texture coordinates to get the texture coordinates of the pixel. This is done by doing a linear combination of the texture coordinates and the barycentric weights.</p>
    
    <h3>Method 1: Nearest Neighbor Sampling</h3>
    <p>However, in the texture space, it is more likely that the coordinate we get is not at the center of a pixel. One way to handle this is to sample the nearest pixel. This is called <strong>point sampling</strong> or <strong>nearest neighbor sampling</strong>. To do this, we simply access the texture at the right level of the mipmap (we will discuss mipmaps later).</p>

    <p>To do this, we first have the coordiantes in <code>uv</code> space for the texture. Then knowing the <code>XY</code> width and height of the texture, we can convert this to the <code>XY</code> coordinates of the texture. The equation however is not straightforward. At first, we might think to multiply <code>uv</code> by <code>xy - 1</code>, subtract 0.5, then round it. However, to handle the edge cases, we want <code>uv=0</code> to access pixel <code>0</code> and <code>uv=1</code> to access pixel <code>xy-1</code>. This results in first having to add <code>1</code> before rounding, then subtracting <code>1</code> after rounding. So we get the following in <code>XY</code> space of the texture:</p>

    <pre><code>round(1 + uv.x*(width - 1) - 0.5) - 1; 
round(1 + uv.y*(height - 1) - 0.5) - 1;</code></pre>

    <p>We then access the texture at the right level of the mipmap.</p>

    <h3>Method 2: Bilinear Interpolation</h3>
    <p>However, if we only sample without any other considerations, we will get aliasing of jaggies. One solution that helps reduce jaggies is <strong>bilinear interpolation</strong>. The below image shows the difference between nearest neighbor sampling and bilinear interpolation with clear jaggies for nearest neighbor, while the second image illustrates how bilinear interpolation works.</p>

    <table class="table-2-col">
        <tr>
            <td><img src="images/image-12.png" alt="Sampling comparison"></td>
            <td><img src="images/image-13.png" alt="Bilinear interpolation illustration"></td>
        </tr>
    </table>

    <p>As shown in the second image, this is done by sampling the 4 nearest pixels and then interpolating the color based on the distance to the 4 pixels. Then we linearly interpolate the color based on the distance to the 4 pixels if they exist.</p>

    <h3>Results</h3>
    <p>The difference between nearest neighbor sampling and bilinear interpolation can be seen in the following images. The first image is nearest neighbor sampling and the second image is bilinear interpolation. We can see the white grid lines clearly in the second image, whereas it looks cut off from the rest of the grid lines in the first image. This is because the second image is interpolating the color.</p>

    <p><strong>sample rate 1</strong></p>
    <table class="table-2-col">
        <tr>
            <th>Nearest Neighbor Sampling</th>
            <th>Bilinear Interpolation</th>
        </tr>
        <tr>
            <td><img src="images/image-14.png" alt="Nearest neighbor at sample rate 1"></td>
            <td><img src="images/image-15.png" alt="Bilinear at sample rate 1"></td>
        </tr>
    </table>

    <p>These are both in sample rate 1. However, note that supersampling can also help to reduce the same aliasing effect that bilinear interpolation does. Here is an example of both methods at sample rate 16. There is not that much difference in this case. Bilinear interpolation is still better as it is more visually consistent and smooth showing the white grid, and it is also cheaper to compute than supersampling.</p>

    <p><strong>sample rate 16</strong></p>
    <table class="table-2-col">
        <tr>
            <th>Nearest Neighbor Sampling</th>
            <th>Bilinear Interpolation</th>
        </tr>
        <tr>
            <td><img src="images/image-16.png" alt="Nearest neighbor at sample rate 16"></td>
            <td><img src="images/image-17.png" alt="Bilinear at sample rate 16"></td>
        </tr>
    </table>

    <p>There will be a large difference between the two methods in general when we are magnifying pixels as bilinear interpolation will smooth it out, and when there are obvious jaggies, since bilinear interpolation will smooth it out. There will also be a large difference on high contrast boundaries, as nearest neighbor sampling will show the boundary clearly, while bilinear interpolation will create a gradient of intermediate values, reducing the contrast but making it look smoother. The core difference is that nearest picks a single texel value, while bilinear averages four neighboring texels. This averaging creates smoother transitions but can blur fine details.</p>

    <h2>Task 6: "Level sampling" with mipmaps for texture mapping</h2>

    <p>Point sampling unfortunately still causes aliasing such as Moire effects for far away textures. This is because the texture is being sampled at a single point at a single resolution, and the texture is being stretched out. Here is an example (from class lecture slides) that shows the aliasing effect when simply doing point sampling:</p>

    <img src="images/image-11.png" alt="Moire effects example" class="single-image">

    <p>The solution to this is to use <strong>Mipmaps</strong> and sample at different levels of the mipmap. Mipmaps are a series of images that are each half the resolution of the previous image. This is done by repeatedly downsampling the image by a factor of 2. For spaces in the screen space that spans very small areas in texture space, we should sample at the level of mipmap that is more zoomed in. For spaces in the screen space that spans very large areas in texture space, we should sample at the level of mipmap that is more zoomed out.</p>

    <img src="images/image-19.png" alt="Mipmap levels" class="single-image">

    <h3>Understanding MIPMAP level calculation</h3>
    <p>Essentially, every single pixel in screen space not only maps to a pixel in texture space, but it also maps to a level of the mipmap. We can calculate this mipmap level with a formula. But to understand the formula, it is important to intuitively recognize the following. Let us define higher level mipmaps as more coarse and lower level mipmaps as more fine (or zoomed in). Then, if we consider a single pixel <code>(px, py)</code> and <code>(px+1, py)</code> and <code>(px, py+1)</code> in screen space then caluculate their corresponding texture coordinates, we have a measure of how much the texture is being stretched out at that point. In other words, we have the "footprint" of the pixel in the <code>x</code> and <code>y</code> direction in texture space. The larger the footprint, the more the texture is being stretched out, and the more coarse the mipmap level should be.</p>

    <p>The math behind it is described as follows:</p>
    <img src="images/image-18.png" alt="Mipmap level calculation" class="single-image">

    <p>The differential norms measure the footprint of the pixel in the <code>x</code> and <code>y</code> direction in texture space. The <code>max</code> function is used to take the largest footprint to account for the worst case (most stretching) direction. The <code>log2</code> function is used to convert the footprint to a level of the mipmap. Specifically, level 0 when footprint is <code><=1</code>, level 1 when footprint is <code><=2</code>, level 2 when footprint is <code><=4</code>, and so on. We implement this as a function <code>get_level</code> as part of the <code>Texture</code> class.</p>

    <h3>Methodology</h3>
    <p>Therefore, we first calculate the texture uv coordinates of the pixel in screen space, and of the pixel to the right and below it. Then we calculate the footprint of the pixel in the <code>x</code> and <code>y</code> direction in texture space. We then have to scale the footprint by the width and height of the texture to get the footprint in texture xy space.</p>

    <p>Then we calculate the mipmap level using the formula above. Then we sample the texture at the calculated mipmap level according to either nearest neighbor sampling or bilinear interpolation.</p>

    <h3>Sampling Methods and Tradeoffs</h3>
    <p>We also implemented three different Level Sampling methods. The first is <em>zero</em>, which means we sample from the zero level mipmap, not taking into account the footprint. The second is <em>nearest</em>, which means we sample from the nearest mipmap level, since the level we calculate is not necessarily an integer. The third is <em>linear</em>, which means we sample from the two nearest mipmap levels and linearly interpolate between them.</p>

    <p>There are tradeoffs between the three techniques in terms of speed, memory usage, and antialiasing power. Specifically, <em>zero</em> is the fastest and uses the least memory, but it does not antialias. <em>linear</em> is the slowest and uses the most memory, but it antialiases the most. <em>nearest</em> is in between the two in terms of speed and memory usage, but it antialiases more than <em>zero</em> but less than <em>linear</em>.</p>

    <h3>Results and Discussion</h3>

    <p>In the following images, I show with combinations of different level sampling methods along with bilinear interpolation and nearest neighbor sampling. It is clear that bilinear interpolation with bilinear sampling is the best in terms of antialiasing. You can see that at some point, the squares turn into gibberish, but it is less seen as gibberish and more of a blur when using bilinear interpolation and bilinear sampling.</p>

    <p><em>Key: <code>L</code> is level sampling method, <code>P</code> is pixel sampling method. <code>L_ZERO</code> is zero, <code>L_NEAREST</code> is nearest, <code>L_LINEAR</code> is linear. <code>P_NEAREST</code> is nearest neighbor, <code>P_LINEAR</code> is bilinear interpolation.</em></p>

    <table class="table-4-col">
        <tr>
            <th>L_ZERO and P_NEAREST</th>
            <th>L_ZERO and P_LINEAR</th>
            <th>L_NEAREST and P_NEAREST</th>
            <th>L_NEAREST and P_LINEAR</th>
        </tr>
        <tr>
            <td><img src="images/l_zero_p_nearest.png" alt="L_ZERO and P_NEAREST"></td>
            <td><img src="images/l_zero_p_linear.png" alt="L_ZERO and P_LINEAR"></td>
            <td><img src="images/l_nearest_p_nearest.png" alt="L_NEAREST and P_NEAREST"></td>
            <td><img src="images/l_nearest_p_linear.png" alt="L_NEAREST and P_LINEAR"></td>
        </tr>
    </table>